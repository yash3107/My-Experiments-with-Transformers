# -*- coding: utf-8 -*-
"""SPPY.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dORN9eRlDBMZ3C9Gy3P62OANiy2noEwN
"""

pip install tensorflow transformers keras datasets

import os
import tensorflow as tf
from datasets import load_dataset
from transformers import TFT5ForConditionalGeneration, T5Tokenizer
import keras

# Load text dataset
dataset = load_dataset("text", data_files={"train": r"shakespeare.txt"})

# Load tokenizer and set the padding token
tokenizer = T5Tokenizer.from_pretrained("t5-small")
tokenizer.pad_token = tokenizer.eos_token

# Tokenization function (remove return_tensors="np" for HF dataset compatibility)
def tokenize_function(examples):
    # Tokenize both input and target (using input as target for this example)
    inputs = tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=512
    )
    # Since it's a text generation task, you can use the input as the target
    # In a real scenario, you would have a separate target text
    inputs["labels"] = inputs["input_ids"].copy()
    return inputs

# Apply tokenization to dataset
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Convert dataset to TensorFlow format
def convert_to_tf_dataset(ds):
    input_ids = tf.ragged.constant(ds["input_ids"], dtype=tf.int32).to_tensor() # Convert ragged tensors to dense tensors.
    attention_mask = tf.ragged.constant(ds["attention_mask"], dtype=tf.int32).to_tensor()
    # Add labels to the dataset
    labels = tf.ragged.constant(ds["labels"], dtype=tf.int32).to_tensor()
    # Add decoder_input_ids to the dataset - this is crucial for T5
    decoder_input_ids = tf.ragged.constant(ds["input_ids"], dtype=tf.int32).to_tensor() # Assuming you use input as target

    # Return a dictionary with the required inputs
    return tf.data.Dataset.from_tensor_slices(({"input_ids": input_ids, "attention_mask": attention_mask, "decoder_input_ids": decoder_input_ids}, labels))

tf_train_dataset = convert_to_tf_dataset(tokenized_datasets["train"]).batch(8).prefetch(tf.data.AUTOTUNE)

import tensorflow as tf
from transformers import TFT5ForConditionalGeneration, T5Tokenizer, AdamWeightDecay, AutoConfig

# Load tokenizer and model
tokenizer = T5Tokenizer.from_pretrained("t5-small")
Yash = TFT5ForConditionalGeneration.from_pretrained("t5-small", num_layers=3)
print(Yash.config)

# Ensure padding token is set
Yash.config.pad_token_id = tokenizer.eos_token_id

# Use Hugging Faceâ€™s AdamW optimizer with weight decay
optimizer = AdamWeightDecay(learning_rate=5e-5, weight_decay_rate=0.01)

# Define loss function (use CategoricalCrossentropy)
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# **Do NOT compile the model (T5 models are not standard Keras models)**
# Instead, train using `.fit()` directly
Yash.compile(optimizer=optimizer, loss=loss_fn)

# Train the model
Yash.fit(tf_train_dataset, epochs=1)

def generate_text(prompt, max_length=1, num_beams=1, num_return_sequences=1):
    input_ids = tokenizer(prompt, return_tensors="tf").input_ids
    output = Yash.generate(
        input_ids,
        max_length=max_length + len(input_ids[0]),  # Ensure it generates only the next 10 words
        num_beams=num_beams,  # Beam search
        num_return_sequences=num_return_sequences  # Generate multiple sequences
    )
    generated_texts = [tokenizer.decode(o, skip_special_tokens=True) for o in output]
    return generated_texts

# Example: Generate Shakespeare-style text
prompt = "To be or not to be, that is"
t = 10  # Number of iterations
generated_prompt = prompt

while t > 0:
    generated_texts = generate_text(prompt+generated_prompt)
    generated_prompt = generated_texts[0]  # Use the first generated sequence for continuity
    t -= 1  # Decrement t

print("Shakespeare-Style Outputs:")
for text in generated_texts:
    print(text)